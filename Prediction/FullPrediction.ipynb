{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "FullPrediction.ipynb",
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "sun46_SiUFXc",
    "outputId": "a8df9292-be92-4aad-9a67-fcd204b7f534"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load info\n",
    "team_season = pd.read_csv(r\"/content/team_season.csv\")\n",
    "team_season.head()"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>year</th>\n",
       "      <th>leag</th>\n",
       "      <th>o_fgm</th>\n",
       "      <th>o_fga</th>\n",
       "      <th>o_ftm</th>\n",
       "      <th>o_fta</th>\n",
       "      <th>o_oreb</th>\n",
       "      <th>o_dreb</th>\n",
       "      <th>o_reb</th>\n",
       "      <th>o_asts</th>\n",
       "      <th>o_pf</th>\n",
       "      <th>o_stl</th>\n",
       "      <th>o_to</th>\n",
       "      <th>o_blk</th>\n",
       "      <th>o_3pm</th>\n",
       "      <th>o_3pa</th>\n",
       "      <th>o_pts</th>\n",
       "      <th>d_fgm</th>\n",
       "      <th>d_fga</th>\n",
       "      <th>d_ftm</th>\n",
       "      <th>d_fta</th>\n",
       "      <th>d_oreb</th>\n",
       "      <th>d_dreb</th>\n",
       "      <th>d_reb</th>\n",
       "      <th>d_asts</th>\n",
       "      <th>d_pf</th>\n",
       "      <th>d_stl</th>\n",
       "      <th>d_to</th>\n",
       "      <th>d_blk</th>\n",
       "      <th>d_3pm</th>\n",
       "      <th>d_3pa</th>\n",
       "      <th>d_pts</th>\n",
       "      <th>pace</th>\n",
       "      <th>won</th>\n",
       "      <th>lost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BOS</td>\n",
       "      <td>1946</td>\n",
       "      <td>N</td>\n",
       "      <td>1397</td>\n",
       "      <td>5133</td>\n",
       "      <td>811</td>\n",
       "      <td>1375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>470</td>\n",
       "      <td>1202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3605</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CH1</td>\n",
       "      <td>1946</td>\n",
       "      <td>N</td>\n",
       "      <td>1879</td>\n",
       "      <td>6309</td>\n",
       "      <td>939</td>\n",
       "      <td>1550</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>436</td>\n",
       "      <td>1473</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4697</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CL1</td>\n",
       "      <td>1946</td>\n",
       "      <td>N</td>\n",
       "      <td>1674</td>\n",
       "      <td>5699</td>\n",
       "      <td>903</td>\n",
       "      <td>1428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>494</td>\n",
       "      <td>1246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4251</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE1</td>\n",
       "      <td>1946</td>\n",
       "      <td>N</td>\n",
       "      <td>1437</td>\n",
       "      <td>5843</td>\n",
       "      <td>923</td>\n",
       "      <td>1494</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>482</td>\n",
       "      <td>1351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3797</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NYK</td>\n",
       "      <td>1946</td>\n",
       "      <td>N</td>\n",
       "      <td>1465</td>\n",
       "      <td>5255</td>\n",
       "      <td>951</td>\n",
       "      <td>1438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>457</td>\n",
       "      <td>1218</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3881</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3840</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  team  year leag  o_fgm  o_fga  o_ftm  ...  d_3pm  d_3pa  d_pts  pace  won  lost\n",
       "0  BOS  1946    N   1397   5133    811  ...      0      0   3900   0.0   22    38\n",
       "1  CH1  1946    N   1879   6309    939  ...      0      0   4471   0.0   39    22\n",
       "2  CL1  1946    N   1674   5699    903  ...      0      0   4308   0.0   30    30\n",
       "3  DE1  1946    N   1437   5843    923  ...      0      0   3918   0.0   20    40\n",
       "4  NYK  1946    N   1465   5255    951  ...      0      0   3840   0.0   33    27\n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j10KRV1GUFXd",
    "outputId": "408dab1d-f8c9-4d68-e4f7-61f69fd42f7a"
   },
   "source": [
    "Y = []\n",
    "for value in team_season.values:\n",
    "    won = value[34]\n",
    "    lost = value[35]\n",
    "    Y.append(float(won / (won + lost)))\n",
    "print(len(Y))"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1187\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-o4K0lvRUFXe",
    "outputId": "0d463728-ab33-4d0b-b6b1-1aa8d72a11a7"
   },
   "source": [
    "print(Y)"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.36666666666666664, 0.639344262295082, 0.5, 0.3333333333333333, 0.55, 0.5833333333333334, 0.25, 0.4666666666666667, 0.6229508196721312, 0.36666666666666664, 0.8166666666666667, 0.5833333333333334, 0.4166666666666667, 0.5833333333333334, 0.5416666666666666, 0.5625, 0.125, 0.6041666666666666, 0.5833333333333334, 0.48333333333333334, 0.4166666666666667, 0.6333333333333333, 0.36666666666666664, 0.3, 0.7333333333333333, 0.5333333333333333, 0.4666666666666667, 0.2, 0.75, 0.48333333333333334, 0.6333333333333333, 0.578125, 0.36764705882352944, 0.3235294117647059, 0.5882352941176471, 0.1774193548387097, 0.5882352941176471, 0.609375, 0.75, 0.5882352941176471, 0.38235294117647056, 0.75, 0.3548387096774194, 0.38235294117647056, 0.796875, 0.453125, 0.3064516129032258, 0.47058823529411764, 0.36363636363636365, 0.5652173913043478, 0.47058823529411764, 0.45588235294117646, 0.6470588235294118, 0.5454545454545454, 0.6060606060606061, 0.6029411764705882, 0.48484848484848486, 0.36764705882352944, 0.2857142857142857, 0.30303030303030304, 0.5909090909090909, 0.4393939393939394, 0.5151515151515151, 0.25757575757575757, 0.6060606060606061, 0.5606060606060606, 0.5, 0.6212121212121212, 0.6060606060606061, 0.22857142857142856, 0.647887323943662, 0.5217391304347826, 0.39436619718309857, 0.38028169014084506, 0.6857142857142857, 0.6714285714285714, 0.17391304347826086, 0.6285714285714286, 0.6619718309859155, 0.2222222222222222, 0.5833333333333334, 0.5555555555555556, 0.2916666666666667, 0.6388888888888888, 0.6111111111111112, 0.4027777777777778, 0.6111111111111112, 0.5833333333333334, 0.5, 0.5972222222222222, 0.3611111111111111, 0.5555555555555556, 0.5277777777777778, 0.4583333333333333, 0.4027777777777778, 0.5972222222222222, 0.5416666666666666, 0.5138888888888888, 0.4583333333333333, 0.4861111111111111, 0.625, 0.4305555555555556, 0.4583333333333333, 0.4861111111111111, 0.6111111111111112, 0.4722222222222222, 0.4722222222222222, 0.5, 0.5138888888888888, 0.4305555555555556, 0.4722222222222222, 0.5277777777777778, 0.6805555555555556, 0.4583333333333333, 0.4583333333333333, 0.2638888888888889, 0.4861111111111111, 0.5138888888888888, 0.5694444444444444, 0.5694444444444444, 0.7222222222222222, 0.2638888888888889, 0.3888888888888889, 0.4583333333333333, 0.5555555555555556, 0.4444444444444444, 0.6805555555555556, 0.4861111111111111, 0.7866666666666666, 0.25333333333333335, 0.4, 0.3333333333333333, 0.36, 0.6533333333333333, 0.6133333333333333, 0.6, 0.7215189873417721, 0.4177215189873418, 0.43037974683544306, 0.45569620253164556, 0.26582278481012656, 0.5822784810126582, 0.6455696202531646, 0.4810126582278481, 0.75, 0.225, 0.5375, 0.4625, 0.675, 0.3625, 0.6125, 0.3625, 0.5125, 0.725, 0.3125, 0.525, 0.425, 0.6625, 0.2625, 0.3875, 0.6, 0.6, 0.3875, 0.7375, 0.6875, 0.2875, 0.525, 0.275, 0.425, 0.6, 0.575, 0.4625, 0.775, 0.6, 0.3875, 0.6125, 0.3875, 0.5, 0.2125, 0.5625, 0.475, 0.675, 0.5625, 0.275, 0.5625, 0.375, 0.6875, 0.4375, 0.45, 0.24691358024691357, 0.7407407407407407, 0.4074074074074074, 0.48148148148148145, 0.37037037037037035, 0.4444444444444444, 0.4444444444444444, 0.8395061728395061, 0.5432098765432098, 0.48148148148148145, 0.32051282051282054, 0.43902439024390244, 0.6585365853658537, 0.35365853658536583, 0.47560975609756095, 0.5769230769230769, 0.4878048780487805, 0.5897435897435898, 0.3717948717948718, 0.48717948717948717, 0.46153846153846156, 0.6341463414634146, 0.6410256410256411, 0.46153846153846156, 0.6153846153846154, 0.524390243902439, 0.28205128205128205, 0.7560975609756098, 0.6923076923076923, 0.18292682926829268, 0.2804878048780488, 0.524390243902439, 0.6829268292682927, 0.5853658536585366, 0.6951219512195121, 0.5853658536585366, 0.4024390243902439, 0.5, 0.5641025641025641, 0.3902439024390244, 0.5256410256410257, 0.2948717948717949, 0.5641025641025641, 0.5384615384615384, 0.6707317073170732, 0.4230769230769231, 0.5512820512820513, 0.32926829268292684, 0.46153846153846156, 0.5897435897435898, 0.6585365853658537, 0.21794871794871795, 0.7692307692307693, 0.6707317073170732, 0.1951219512195122, 0.45121951219512196, 0.36585365853658536, 0.5, 0.5853658536585366, 0.6097560975609756, 0.4146341463414634, 0.5, 0.47560975609756095, 0.43902439024390244, 0.6071428571428571, 0.3780487804878049, 0.5357142857142857, 0.7023809523809523, 0.5357142857142857, 0.5609756097560976, 0.5119047619047619, 0.27380952380952384, 0.6829268292682927, 0.5, 0.7317073170731707, 0.4642857142857143, 0.5121951219512195, 0.47560975609756095, 0.34523809523809523, 0.32926829268292684, 0.43902439024390244, 0.36585365853658536, 0.5238095238095238, 0.43902439024390244, 0.5121951219512195, 0.5365853658536586, 0.2682926829268293, 0.40476190476190477, 0.6219512195121951, 0.4024390243902439, 0.18292682926829268, 0.35714285714285715, 0.5487804878048781, 0.44047619047619047, 0.6904761904761905, 0.5238095238095238, 0.5853658536585366, 0.8048780487804879, 0.4880952380952381, 0.6341463414634146, 0.47619047619047616, 0.573170731707317, 0.5853658536585366, 0.35365853658536583, 0.42857142857142855, 0.4878048780487805, 0.4634146341463415, 0.5, 0.35714285714285715, 0.6785714285714286, 0.6547619047619048, 0.43902439024390244, 0.4634146341463415, 0.6829268292682927, 0.2682926829268293, 0.4166666666666667, 0.6951219512195121, 0.36585365853658536, 0.2804878048780488, 0.40476190476190477, 0.3170731707317073, 0.5, 0.42857142857142855, 0.6219512195121951, 0.4146341463414634, 0.5595238095238095, 0.8095238095238095, 0.8414634146341463, 0.7682926829268293, 0.30952380952380953, 0.5853658536585366, 0.5238095238095238, 0.36585365853658536, 0.5975609756097561, 0.21951219512195122, 0.2976190476190476, 0.573170731707317, 0.7142857142857143, 0.5357142857142857, 0.5609756097560976, 0.6341463414634146, 0.8292682926829268, 0.25609756097560976, 0.6785714285714286, 0.6219512195121951, 0.3902439024390244, 0.5595238095238095, 0.4878048780487805, 0.3333333333333333, 0.573170731707317, 0.4024390243902439, 0.6071428571428571, 0.43902439024390244, 0.6666666666666666, 0.7317073170731707, 0.7317073170731707, 0.2857142857142857, 0.6951219512195121, 0.35714285714285715, 0.10975609756097561, 0.4634146341463415, 0.25609756097560976, 0.35714285714285715, 0.3170731707317073, 0.6547619047619048, 0.5, 0.4268292682926829, 0.6829268292682927, 0.5121951219512195, 0.573170731707317, 0.5595238095238095, 0.6585365853658537, 0.35365853658536583, 0.44047619047619047, 0.6341463414634146, 0.5365853658536586, 0.3902439024390244, 0.5476190476190477, 0.4024390243902439, 0.6309523809523809, 0.573170731707317, 0.7195121951219512, 0.25, 0.5975609756097561, 0.6547619047619048, 0.3048780487804878, 0.36585365853658536, 0.32926829268292684, 0.5357142857142857, 0.44047619047619047, 0.43902439024390244, 0.6071428571428571, 0.3333333333333333, 0.3780487804878049, 0.7317073170731707, 0.5975609756097561, 0.573170731707317, 0.4878048780487805, 0.7738095238095238, 0.4878048780487805, 0.5853658536585366, 0.5, 0.5357142857142857, 0.5365853658536586, 0.6904761904761905, 0.36585365853658536, 0.4634146341463415, 0.32142857142857145, 0.2804878048780488, 0.4878048780487805, 0.6904761904761905, 0.4146341463414634, 0.3902439024390244, 0.4634146341463415, 0.6071428571428571, 0.36904761904761907, 0.524390243902439, 0.38095238095238093, 0.4523809523809524, 0.17857142857142858, 0.7317073170731707, 0.35365853658536583, 0.6585365853658537, 0.5609756097560976, 0.2926829268292683, 0.5975609756097561, 0.7142857142857143, 0.43902439024390244, 0.7195121951219512, 0.4878048780487805, 0.4642857142857143, 0.3780487804878049, 0.5476190476190477, 0.4878048780487805, 0.4634146341463415, 0.4634146341463415, 0.4634146341463415, 0.6547619047619048, 0.5609756097560976, 0.5121951219512195, 0.45121951219512196, 0.5952380952380952, 0.2727272727272727, 0.524390243902439, 0.4166666666666667, 0.25, 0.18072289156626506, 0.5853658536585366, 0.3780487804878049, 0.5365853658536586, 0.36585365853658536, 0.5365853658536586, 0.524390243902439, 0.6097560975609756, 0.5365853658536586, 0.5609756097560976, 0.5975609756097561, 0.43902439024390244, 0.4878048780487805, 0.6463414634146342, 0.36585365853658536, 0.4268292682926829, 0.4878048780487805, 0.2682926829268293, 0.6097560975609756, 0.4146341463414634, 0.5975609756097561, 0.5365853658536586, 0.4878048780487805, 0.5853658536585366, 0.5, 0.3902439024390244, 0.32926829268292684, 0.4878048780487805, 0.524390243902439, 0.5853658536585366, 0.4634146341463415, 0.524390243902439, 0.34146341463414637, 0.3780487804878049, 0.3780487804878049, 0.5487804878048781, 0.5365853658536586, 0.2926829268292683, 0.47560975609756095, 0.524390243902439, 0.6707317073170732, 0.5975609756097561, 0.7073170731707317, 0.6341463414634146, 0.573170731707317, 0.5365853658536586, 0.5609756097560976, 0.35365853658536583, 0.3780487804878049, 0.36585365853658536, 0.573170731707317, 0.36585365853658536, 0.4634146341463415, 0.573170731707317, 0.4634146341463415, 0.5853658536585366, 0.573170731707317, 0.4634146341463415, 0.45121951219512196, 0.3170731707317073, 0.3780487804878049, 0.573170731707317, 0.6097560975609756, 0.5487804878048781, 0.5853658536585366, 0.524390243902439, 0.6341463414634146, 0.6585365853658537, 0.6097560975609756, 0.7439024390243902, 0.36585365853658536, 0.45121951219512196, 0.36585365853658536, 0.1951219512195122, 0.2926829268292683, 0.5, 0.45121951219512196, 0.573170731707317, 0.7317073170731707, 0.5975609756097561, 0.4146341463414634, 0.47560975609756095, 0.7195121951219512, 0.6707317073170732, 0.4634146341463415, 0.5, 0.4268292682926829, 0.6829268292682927, 0.2926829268292683, 0.47560975609756095, 0.3780487804878049, 0.7560975609756098, 0.5487804878048781, 0.34146341463414637, 0.18292682926829268, 0.45121951219512196, 0.25609756097560976, 0.47560975609756095, 0.4878048780487805, 0.5365853658536586, 0.4878048780487805, 0.6585365853658537, 0.7317073170731707, 0.2926829268292683, 0.6097560975609756, 0.7560975609756098, 0.6951219512195121, 0.5487804878048781, 0.6341463414634146, 0.43902439024390244, 0.4146341463414634, 0.34146341463414637, 0.47560975609756095, 0.5121951219512195, 0.7682926829268293, 0.4146341463414634, 0.18292682926829268, 0.34146341463414637, 0.5609756097560976, 0.47560975609756095, 0.5487804878048781, 0.5609756097560976, 0.4268292682926829, 0.36585365853658536, 0.6951219512195121, 0.6707317073170732, 0.5365853658536586, 0.4024390243902439, 0.7073170731707317, 0.5609756097560976, 0.5121951219512195, 0.5853658536585366, 0.2073170731707317, 0.6341463414634146, 0.3048780487804878, 0.524390243902439, 0.524390243902439, 0.6829268292682927, 0.34146341463414637, 0.2804878048780488, 0.4634146341463415, 0.5487804878048781, 0.45121951219512196, 0.36585365853658536, 0.17073170731707318, 0.24390243902439024, 0.5487804878048781, 0.7073170731707317, 0.6219512195121951, 0.5975609756097561, 0.5365853658536586, 0.7926829268292683, 0.6463414634146342, 0.5609756097560976, 0.6463414634146342, 0.3048780487804878, 0.5853658536585366, 0.36585365853658536, 0.5121951219512195, 0.4878048780487805, 0.7560975609756098, 0.32926829268292684, 0.34146341463414637, 0.524390243902439, 0.4634146341463415, 0.5975609756097561, 0.45121951219512196, 0.35365853658536583, 0.3170731707317073, 0.4634146341463415, 0.6585365853658537, 0.6097560975609756, 0.5487804878048781, 0.573170731707317, 0.6341463414634146, 0.5, 0.5853658536585366, 0.45121951219512196, 0.36585365853658536, 0.5121951219512195, 0.5487804878048781, 0.4268292682926829, 0.4146341463414634, 0.7682926829268293, 0.4634146341463415, 0.43902439024390244, 0.5365853658536586, 0.6341463414634146, 0.5609756097560976, 0.2682926829268293, 0.5853658536585366, 0.2682926829268293, 0.3780487804878049, 0.3780487804878049, 0.7560975609756098, 0.7195121951219512, 0.5121951219512195, 0.2926829268292683, 0.7073170731707317, 0.43902439024390244, 0.5121951219512195, 0.5, 0.3780487804878049, 0.5, 0.4878048780487805, 0.6097560975609756, 0.8170731707317073, 0.36585365853658536, 0.35365853658536583, 0.5365853658536586, 0.573170731707317, 0.5609756097560976, 0.36585365853658536, 0.6219512195121951, 0.3170731707317073, 0.3902439024390244, 0.7560975609756098, 0.6951219512195121, 0.47560975609756095, 0.2804878048780488, 0.6585365853658537, 0.3902439024390244, 0.4878048780487805, 0.45121951219512196, 0.4268292682926829, 0.3780487804878049, 0.5121951219512195, 0.47560975609756095, 0.6951219512195121, 0.7195121951219512, 0.4878048780487805, 0.3780487804878049, 0.6707317073170732, 0.45121951219512196, 0.6341463414634146, 0.5121951219512195, 0.5121951219512195, 0.5, 0.14634146341463414, 0.7926829268292683, 0.6097560975609756, 0.2926829268292683, 0.2926829268292683, 0.5487804878048781, 0.43902439024390244, 0.5975609756097561, 0.35365853658536583, 0.34146341463414637, 0.47560975609756095, 0.5365853658536586, 0.5121951219512195, 0.6097560975609756, 0.6951219512195121, 0.6097560975609756, 0.5121951219512195, 0.6463414634146342, 0.6585365853658537, 0.6585365853658537, 0.24390243902439024, 0.5609756097560976, 0.4634146341463415, 0.2073170731707317, 0.7560975609756098, 0.5121951219512195, 0.23170731707317074, 0.4634146341463415, 0.43902439024390244, 0.34146341463414637, 0.6463414634146342, 0.2926829268292683, 0.3780487804878049, 0.5365853658536586, 0.573170731707317, 0.4634146341463415, 0.6341463414634146, 0.5121951219512195, 0.24390243902439024, 0.573170731707317, 0.6951219512195121, 0.4634146341463415, 0.5365853658536586, 0.7682926829268293, 0.524390243902439, 0.5487804878048781, 0.34146341463414637, 0.25609756097560976, 0.6951219512195121, 0.18292682926829268, 0.5975609756097561, 0.3170731707317073, 0.6341463414634146, 0.5609756097560976, 0.6707317073170732, 0.47560975609756095, 0.32926829268292684, 0.25609756097560976, 0.573170731707317, 0.6219512195121951, 0.4878048780487805, 0.5, 0.6341463414634146, 0.23170731707317074, 0.6707317073170732, 0.5121951219512195, 0.573170731707317, 0.524390243902439, 0.7195121951219512, 0.45121951219512196, 0.5, 0.5121951219512195, 0.36585365853658536, 0.7682926829268293, 0.21951219512195122, 0.5365853658536586, 0.2682926829268293, 0.2073170731707317, 0.5487804878048781, 0.21951219512195122, 0.6463414634146342, 0.6585365853658537, 0.7195121951219512, 0.2804878048780488, 0.6829268292682927, 0.5, 0.6707317073170732, 0.3780487804878049, 0.524390243902439, 0.6829268292682927, 0.3170731707317073, 0.7439024390243902, 0.4024390243902439, 0.34146341463414637, 0.24390243902439024, 0.6097560975609756, 0.5365853658536586, 0.6341463414634146, 0.5, 0.3780487804878049, 0.7073170731707317, 0.2926829268292683, 0.5853658536585366, 0.35365853658536583, 0.3170731707317073, 0.47560975609756095, 0.3780487804878049, 0.5365853658536586, 0.6707317073170732, 0.7682926829268293, 0.3048780487804878, 0.6707317073170732, 0.5, 0.6585365853658537, 0.36585365853658536, 0.4634146341463415, 0.6219512195121951, 0.3780487804878049, 0.8170731707317073, 0.6951219512195121, 0.2682926829268293, 0.2926829268292683, 0.5853658536585366, 0.6707317073170732, 0.5121951219512195, 0.4878048780487805, 0.5487804878048781, 0.524390243902439, 0.4634146341463415, 0.3780487804878049, 0.18292682926829268, 0.4878048780487805, 0.6219512195121951, 0.25609756097560976, 0.4268292682926829, 0.6463414634146342, 0.6951219512195121, 0.35365853658536583, 0.573170731707317, 0.573170731707317, 0.6707317073170732, 0.3048780487804878, 0.524390243902439, 0.5853658536585366, 0.5365853658536586, 0.6951219512195121, 0.6585365853658537, 0.13414634146341464, 0.43902439024390244, 0.4878048780487805, 0.4146341463414634, 0.6707317073170732, 0.5, 0.5, 0.47560975609756095, 0.43902439024390244, 0.34146341463414637, 0.23170731707317074, 0.524390243902439, 0.7317073170731707, 0.5, 0.3170731707317073, 0.7560975609756098, 0.6219512195121951, 0.3048780487804878, 0.5975609756097561, 0.6707317073170732, 0.573170731707317, 0.2682926829268293, 0.6951219512195121, 0.3902439024390244, 0.5, 0.6707317073170732, 0.573170731707317, 0.15853658536585366, 0.5121951219512195, 0.24390243902439024, 0.6097560975609756, 0.7073170731707317, 0.573170731707317, 0.32926829268292684, 0.4024390243902439, 0.5121951219512195, 0.24390243902439024, 0.24390243902439024, 0.5487804878048781, 0.6951219512195121, 0.6097560975609756, 0.3048780487804878, 0.6829268292682927, 0.573170731707317, 0.34146341463414637, 0.6707317073170732, 0.7682926829268293, 0.6463414634146342, 0.2926829268292683, 0.5121951219512195, 0.4268292682926829, 0.6097560975609756, 0.573170731707317, 0.524390243902439, 0.43902439024390244, 0.5, 0.34146341463414637, 0.3170731707317073, 0.573170731707317, 0.6341463414634146, 0.2073170731707317, 0.5853658536585366, 0.3902439024390244, 0.4146341463414634, 0.25609756097560976, 0.36585365853658536, 0.6707317073170732, 0.6951219512195121, 0.2926829268292683, 0.7195121951219512, 0.5365853658536586, 0.47560975609756095, 0.7560975609756098, 0.6951219512195121, 0.7317073170731707, 0.25609756097560976, 0.5609756097560976, 0.4024390243902439, 0.5, 0.8780487804878049, 0.573170731707317, 0.3170731707317073, 0.4268292682926829, 0.5609756097560976, 0.43902439024390244, 0.5853658536585366, 0.6341463414634146, 0.35365853658536583, 0.6463414634146342, 0.5121951219512195, 0.3048780487804878, 0.3170731707317073, 0.36585365853658536, 0.573170731707317, 0.7317073170731707, 0.21951219512195122, 0.5, 0.5365853658536586, 0.47560975609756095, 0.7195121951219512, 0.7804878048780488, 0.25609756097560976, 0.6707317073170732, 0.18292682926829268, 0.47560975609756095, 0.6829268292682927, 0.18292682926829268, 0.6585365853658537, 0.8414634146341463, 0.5121951219512195, 0.2926829268292683, 0.25609756097560976, 0.6585365853658537, 0.36585365853658536, 0.6951219512195121, 0.47560975609756095, 0.43902439024390244, 0.6829268292682927, 0.7439024390243902, 0.4024390243902439, 0.4878048780487805, 0.3170731707317073, 0.6951219512195121, 0.5487804878048781, 0.2682926829268293, 0.4878048780487805, 0.5975609756097561, 0.4146341463414634, 0.24390243902439024, 0.6951219512195121, 0.36585365853658536, 0.7804878048780488, 0.17073170731707318, 0.5365853658536586, 0.6097560975609756, 0.43902439024390244, 0.6219512195121951, 0.7560975609756098, 0.573170731707317, 0.24390243902439024, 0.13414634146341464, 0.45121951219512196, 0.23170731707317074, 0.5, 0.7073170731707317, 0.2073170731707317, 0.7439024390243902, 0.6707317073170732, 0.43902439024390244, 0.5487804878048781, 0.524390243902439, 0.524390243902439, 0.5, 0.3780487804878049, 0.6829268292682927, 0.5609756097560976, 0.32926829268292684, 0.6829268292682927, 0.7439024390243902, 0.1951219512195122, 0.7560975609756098, 0.23170731707317074, 0.5121951219512195, 0.62, 0.38, 0.52, 0.26, 0.44, 0.38, 0.28, 0.58, 0.42, 0.62, 0.66, 0.18, 0.62, 0.66, 0.56, 0.5, 0.32, 0.54, 0.66, 0.56, 0.54, 0.7, 0.54, 0.74, 0.5, 0.46, 0.74, 0.16, 0.36, 0.34146341463414637, 0.4268292682926829, 0.5975609756097561, 0.2073170731707317, 0.3902439024390244, 0.4878048780487805, 0.4268292682926829, 0.5121951219512195, 0.23170731707317074, 0.4146341463414634, 0.6829268292682927, 0.18292682926829268, 0.8170731707317073, 0.6341463414634146, 0.5121951219512195, 0.6097560975609756, 0.3780487804878049, 0.6097560975609756, 0.5, 0.5975609756097561, 0.6463414634146342, 0.7195121951219512, 0.5365853658536586, 0.6463414634146342, 0.5487804878048781, 0.5487804878048781, 0.6707317073170732, 0.2682926829268293, 0.35365853658536583, 0.3048780487804878, 0.43902439024390244, 0.5609756097560976, 0.18292682926829268, 0.36585365853658536, 0.6463414634146342, 0.4878048780487805, 0.3902439024390244, 0.2073170731707317, 0.5487804878048781, 0.5, 0.3780487804878049, 0.6829268292682927, 0.6097560975609756, 0.6341463414634146, 0.573170731707317, 0.3170731707317073, 0.5853658536585366, 0.524390243902439, 0.6829268292682927, 0.6219512195121951, 0.6097560975609756, 0.6707317073170732, 0.7073170731707317, 0.5365853658536586, 0.573170731707317, 0.6463414634146342, 0.2804878048780488, 0.23170731707317074, 0.4024390243902439, 0.5975609756097561, 0.5365853658536586, 0.25609756097560976, 0.35365853658536583, 0.6951219512195121, 0.32926829268292684, 0.6097560975609756, 0.25609756097560976, 0.34146341463414637, 0.5121951219512195, 0.47560975609756095, 0.7073170731707317, 0.2804878048780488, 0.43902439024390244, 0.5, 0.6097560975609756, 0.6341463414634146, 0.36585365853658536, 0.5365853658536586, 0.524390243902439, 0.43902439024390244, 0.5975609756097561, 0.7439024390243902, 0.7073170731707317, 0.5487804878048781, 0.5121951219512195, 0.5365853658536586, 0.45121951219512196, 0.4268292682926829, 0.5365853658536586, 0.36585365853658536, 0.2073170731707317, 0.7317073170731707, 0.2073170731707317, 0.6097560975609756, 0.4634146341463415, 0.524390243902439, 0.5853658536585366, 0.32926829268292684, 0.6097560975609756, 0.34146341463414637, 0.3048780487804878, 0.5121951219512195, 0.6219512195121951, 0.5975609756097561, 0.573170731707317, 0.45121951219512196, 0.5121951219512195, 0.5853658536585366, 0.5365853658536586, 0.6097560975609756, 0.7195121951219512, 0.7317073170731707, 0.4878048780487805, 0.2926829268292683, 0.573170731707317, 0.45121951219512196, 0.34146341463414637, 0.43902439024390244, 0.2804878048780488, 0.4268292682926829, 0.6341463414634146, 0.524390243902439, 0.6585365853658537, 0.45121951219512196, 0.5487804878048781, 0.7439024390243902, 0.34146341463414637, 0.6829268292682927, 0.6097560975609756, 0.5121951219512195, 0.5, 0.7073170731707317, 0.573170731707317, 0.5, 0.47560975609756095, 0.25609756097560976, 0.4024390243902439, 0.35365853658536583, 0.5, 0.6707317073170732, 0.6951219512195121, 0.45121951219512196, 0.4024390243902439, 0.5121951219512195, 0.3048780487804878, 0.15853658536585366, 0.5487804878048781, 0.573170731707317, 0.21951219512195122, 0.5121951219512195, 0.7073170731707317, 0.5975609756097561, 0.6585365853658537, 0.4146341463414634, 0.6219512195121951, 0.5365853658536586, 0.45121951219512196, 0.4146341463414634, 0.5487804878048781, 0.7195121951219512, 0.36585365853658536, 0.5365853658536586, 0.5121951219512195, 0.21951219512195122, 0.4024390243902439, 0.43902439024390244, 0.524390243902439, 0.7560975609756098, 0.32926829268292684, 0.6097560975609756, 0.7195121951219512, 0.6341463414634146, 0.4024390243902439, 0.3170731707317073, 0.5487804878048781]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "ad2jXnGWUFXe",
    "outputId": "38c34ac6-687a-462b-a44b-a189445eeebd"
   },
   "source": [
    "import copy\n",
    "\n",
    "team_season_dropped = copy.deepcopy(team_season)\n",
    "team_season_dropped.drop(team_season_dropped.columns[[0, 1, 2, 34, 35]], axis=1, inplace=True)\n",
    "team_season_dropped.head()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>o_fgm</th>\n",
       "      <th>o_fga</th>\n",
       "      <th>o_ftm</th>\n",
       "      <th>o_fta</th>\n",
       "      <th>o_oreb</th>\n",
       "      <th>o_dreb</th>\n",
       "      <th>o_reb</th>\n",
       "      <th>o_asts</th>\n",
       "      <th>o_pf</th>\n",
       "      <th>o_stl</th>\n",
       "      <th>o_to</th>\n",
       "      <th>o_blk</th>\n",
       "      <th>o_3pm</th>\n",
       "      <th>o_3pa</th>\n",
       "      <th>o_pts</th>\n",
       "      <th>d_fgm</th>\n",
       "      <th>d_fga</th>\n",
       "      <th>d_ftm</th>\n",
       "      <th>d_fta</th>\n",
       "      <th>d_oreb</th>\n",
       "      <th>d_dreb</th>\n",
       "      <th>d_reb</th>\n",
       "      <th>d_asts</th>\n",
       "      <th>d_pf</th>\n",
       "      <th>d_stl</th>\n",
       "      <th>d_to</th>\n",
       "      <th>d_blk</th>\n",
       "      <th>d_3pm</th>\n",
       "      <th>d_3pa</th>\n",
       "      <th>d_pts</th>\n",
       "      <th>pace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1397</td>\n",
       "      <td>5133</td>\n",
       "      <td>811</td>\n",
       "      <td>1375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>470</td>\n",
       "      <td>1202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3605</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1879</td>\n",
       "      <td>6309</td>\n",
       "      <td>939</td>\n",
       "      <td>1550</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>436</td>\n",
       "      <td>1473</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4697</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4471</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1674</td>\n",
       "      <td>5699</td>\n",
       "      <td>903</td>\n",
       "      <td>1428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>494</td>\n",
       "      <td>1246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4251</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4308</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1437</td>\n",
       "      <td>5843</td>\n",
       "      <td>923</td>\n",
       "      <td>1494</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>482</td>\n",
       "      <td>1351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3797</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3918</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>5255</td>\n",
       "      <td>951</td>\n",
       "      <td>1438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>457</td>\n",
       "      <td>1218</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3881</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3840</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   o_fgm  o_fga  o_ftm  o_fta  o_oreb  ...  d_blk  d_3pm  d_3pa  d_pts  pace\n",
       "0   1397   5133    811   1375       0  ...      0      0      0   3900   0.0\n",
       "1   1879   6309    939   1550       0  ...      0      0      0   4471   0.0\n",
       "2   1674   5699    903   1428       0  ...      0      0      0   4308   0.0\n",
       "3   1437   5843    923   1494       0  ...      0      0      0   3918   0.0\n",
       "4   1465   5255    951   1438       0  ...      0      0      0   3840   0.0\n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LwGOjdygUFXe",
    "outputId": "59b2dd5e-a3b0-4f1d-b05e-7a6efd99c725"
   },
   "source": [
    "print(len(team_season_dropped) == len(Y))"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "3n_RPpO1UFXf"
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = StandardScaler().fit_transform(team_season_dropped)"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNhn54PyUFXg",
    "outputId": "f21002ab-9966-480a-8eb3-f3280613def1"
   },
   "source": [
    "print(X[1])"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-2.25879253 -0.76226965 -2.57911991 -1.90160805 -1.39919916 -1.4238284\n",
      " -3.64340751 -3.90768748 -1.75614316 -1.39490271 -1.38999949 -1.3489673\n",
      " -0.87529308 -0.40245799 -2.6678785  -1.90848527 -1.92788372 -1.87532842\n",
      " -1.87783404 -1.40722085 -1.42288148 -1.9175295  -1.89454811 -1.92587031\n",
      " -1.405453   -1.38708193 -1.3863296  -0.92540501 -0.41297301 -2.83857972\n",
      " -1.43668687]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDsqjKrGUFXg",
    "outputId": "5a4f1477-f21c-42e5-e40c-95ac94805406"
   },
   "source": [
    "print(Y[1])"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.639344262295082\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6WYDskdjUFXh"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZoUucZfUFXh",
    "outputId": "6637bc47-6222-46ca-8f95-790b0c581129"
   },
   "source": [
    "x = X\n",
    "y = Y\n",
    "y = np.reshape(y, (-1, 1))\n",
    "scalar_x = MinMaxScaler()\n",
    "scalar_y = MinMaxScaler()\n",
    "print(scalar_x.fit(x))\n",
    "xscale = scalar_x.transform(x)\n",
    "print(scalar_y.fit(y))\n",
    "yscale = scalar_y.transform(y)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MinMaxScaler()\n",
      "MinMaxScaler()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "KMT9hbvRUFXi"
   },
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(xscale, yscale)"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgiO_M62UFXi",
    "outputId": "488e1675-f6a2-4198-f9da-ca5249dfbd01"
   },
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(31, input_dim=31, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 38,241\n",
      "Trainable params: 38,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "z6D39YXHUFXi"
   },
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OI5_Ek1HUFXj",
    "outputId": "fa0abdb0-fc9c-4417-a3dd-a8fd5f53d54b"
   },
   "source": [
    "history = model.fit(X_train, y_train, epochs=250, batch_size=50, verbose=1, validation_split=0.2)"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/250\n",
      "15/15 [==============================] - 3s 22ms/step - loss: 0.1013 - mse: 0.1013 - mae: 0.2534 - val_loss: 0.0481 - val_mse: 0.0481 - val_mae: 0.1899\n",
      "Epoch 2/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0476 - mse: 0.0476 - mae: 0.1791 - val_loss: 0.0386 - val_mse: 0.0386 - val_mae: 0.1632\n",
      "Epoch 3/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0398 - mse: 0.0398 - mae: 0.1607 - val_loss: 0.0409 - val_mse: 0.0409 - val_mae: 0.1600\n",
      "Epoch 4/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0402 - mse: 0.0402 - mae: 0.1645 - val_loss: 0.0353 - val_mse: 0.0353 - val_mae: 0.1537\n",
      "Epoch 5/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0362 - mse: 0.0362 - mae: 0.1553 - val_loss: 0.0381 - val_mse: 0.0381 - val_mae: 0.1549\n",
      "Epoch 6/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0351 - mse: 0.0351 - mae: 0.1526 - val_loss: 0.0357 - val_mse: 0.0357 - val_mae: 0.1512\n",
      "Epoch 7/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0334 - mse: 0.0334 - mae: 0.1490 - val_loss: 0.0323 - val_mse: 0.0323 - val_mae: 0.1509\n",
      "Epoch 8/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0320 - mse: 0.0320 - mae: 0.1474 - val_loss: 0.0329 - val_mse: 0.0329 - val_mae: 0.1437\n",
      "Epoch 9/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0336 - mse: 0.0336 - mae: 0.1477 - val_loss: 0.0379 - val_mse: 0.0379 - val_mae: 0.1691\n",
      "Epoch 10/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0321 - mse: 0.0321 - mae: 0.1472 - val_loss: 0.0293 - val_mse: 0.0293 - val_mae: 0.1430\n",
      "Epoch 11/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0267 - mse: 0.0267 - mae: 0.1331 - val_loss: 0.0254 - val_mse: 0.0254 - val_mae: 0.1321\n",
      "Epoch 12/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0249 - mse: 0.0249 - mae: 0.1282 - val_loss: 0.0258 - val_mse: 0.0258 - val_mae: 0.1302\n",
      "Epoch 13/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0217 - mse: 0.0217 - mae: 0.1177 - val_loss: 0.0204 - val_mse: 0.0204 - val_mae: 0.1148\n",
      "Epoch 14/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0210 - mse: 0.0210 - mae: 0.1164 - val_loss: 0.0192 - val_mse: 0.0192 - val_mae: 0.1113\n",
      "Epoch 15/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0184 - mse: 0.0184 - mae: 0.1067 - val_loss: 0.0251 - val_mse: 0.0251 - val_mae: 0.1311\n",
      "Epoch 16/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0179 - mse: 0.0179 - mae: 0.1056 - val_loss: 0.0141 - val_mse: 0.0141 - val_mae: 0.0934\n",
      "Epoch 17/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0163 - mse: 0.0163 - mae: 0.1012 - val_loss: 0.0150 - val_mse: 0.0150 - val_mae: 0.0957\n",
      "Epoch 18/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0145 - mse: 0.0145 - mae: 0.0936 - val_loss: 0.0163 - val_mse: 0.0163 - val_mae: 0.1037\n",
      "Epoch 19/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0148 - mse: 0.0148 - mae: 0.0970 - val_loss: 0.0156 - val_mse: 0.0156 - val_mae: 0.0967\n",
      "Epoch 20/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0138 - mse: 0.0138 - mae: 0.0894 - val_loss: 0.0122 - val_mse: 0.0122 - val_mae: 0.0858\n",
      "Epoch 21/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0129 - mse: 0.0129 - mae: 0.0876 - val_loss: 0.0142 - val_mse: 0.0142 - val_mae: 0.0931\n",
      "Epoch 22/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0135 - mse: 0.0135 - mae: 0.0902 - val_loss: 0.0117 - val_mse: 0.0117 - val_mae: 0.0834\n",
      "Epoch 23/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0112 - mse: 0.0112 - mae: 0.0815 - val_loss: 0.0154 - val_mse: 0.0154 - val_mae: 0.0989\n",
      "Epoch 24/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0116 - mse: 0.0116 - mae: 0.0824 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0797\n",
      "Epoch 25/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0110 - mse: 0.0110 - mae: 0.0811 - val_loss: 0.0108 - val_mse: 0.0108 - val_mae: 0.0793\n",
      "Epoch 26/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0108 - mse: 0.0108 - mae: 0.0802 - val_loss: 0.0133 - val_mse: 0.0133 - val_mae: 0.0897\n",
      "Epoch 27/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0114 - mse: 0.0114 - mae: 0.0813 - val_loss: 0.0119 - val_mse: 0.0119 - val_mae: 0.0826\n",
      "Epoch 28/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0129 - mse: 0.0129 - mae: 0.0890 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0764\n",
      "Epoch 29/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0106 - mse: 0.0106 - mae: 0.0774 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0770\n",
      "Epoch 30/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0101 - mse: 0.0101 - mae: 0.0761 - val_loss: 0.0118 - val_mse: 0.0118 - val_mae: 0.0837\n",
      "Epoch 31/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0101 - mse: 0.0101 - mae: 0.0760 - val_loss: 0.0151 - val_mse: 0.0151 - val_mae: 0.0999\n",
      "Epoch 32/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0115 - mse: 0.0115 - mae: 0.0833 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0778\n",
      "Epoch 33/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0725 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0793\n",
      "Epoch 34/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0096 - mse: 0.0096 - mae: 0.0760 - val_loss: 0.0119 - val_mse: 0.0119 - val_mae: 0.0876\n",
      "Epoch 35/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0749 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0741\n",
      "Epoch 36/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0746 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0707\n",
      "Epoch 37/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0107 - mse: 0.0107 - mae: 0.0814 - val_loss: 0.0180 - val_mse: 0.0180 - val_mae: 0.1072\n",
      "Epoch 38/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0106 - mse: 0.0106 - mae: 0.0816 - val_loss: 0.0109 - val_mse: 0.0109 - val_mae: 0.0814\n",
      "Epoch 39/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0701 - val_loss: 0.0146 - val_mse: 0.0146 - val_mae: 0.0967\n",
      "Epoch 40/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0102 - mse: 0.0102 - mae: 0.0808 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0737\n",
      "Epoch 41/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0705 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0670\n",
      "Epoch 42/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0649 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0737\n",
      "Epoch 43/250\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0680 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0748\n",
      "Epoch 44/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0679 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0649\n",
      "Epoch 45/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0682 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0807\n",
      "Epoch 46/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0693 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0767\n",
      "Epoch 47/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0635 - val_loss: 0.0087 - val_mse: 0.0087 - val_mae: 0.0717\n",
      "Epoch 48/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0639 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0655\n",
      "Epoch 49/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0615 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0662\n",
      "Epoch 50/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0089 - mse: 0.0089 - mae: 0.0760 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0705\n",
      "Epoch 51/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0693 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0788\n",
      "Epoch 52/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0604 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0857\n",
      "Epoch 53/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0683 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0640\n",
      "Epoch 54/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0631 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0588\n",
      "Epoch 55/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0581 - val_loss: 0.0118 - val_mse: 0.0118 - val_mae: 0.0887\n",
      "Epoch 56/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0630 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0720\n",
      "Epoch 57/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0592 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0770\n",
      "Epoch 58/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0602 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0587\n",
      "Epoch 59/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0631 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0594\n",
      "Epoch 60/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0760 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0673\n",
      "Epoch 61/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0601 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0823\n",
      "Epoch 62/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0678 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0610\n",
      "Epoch 63/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0593 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0666\n",
      "Epoch 64/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0673 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0764\n",
      "Epoch 65/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0603 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0566\n",
      "Epoch 66/250\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0557 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0588\n",
      "Epoch 67/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0540 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0661\n",
      "Epoch 68/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0564 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0583\n",
      "Epoch 69/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0526 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0784\n",
      "Epoch 70/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0599 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0557\n",
      "Epoch 71/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0559 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0790\n",
      "Epoch 72/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0631 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0577\n",
      "Epoch 73/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0550 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0732\n",
      "Epoch 74/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0547 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0589\n",
      "Epoch 75/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0550 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0593\n",
      "Epoch 76/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0529 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0610\n",
      "Epoch 77/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0667 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0630\n",
      "Epoch 78/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0528 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0550\n",
      "Epoch 79/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0040 - mse: 0.0040 - mae: 0.0501 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0529\n",
      "Epoch 80/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0472 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0554\n",
      "Epoch 81/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0495 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0512\n",
      "Epoch 82/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0477 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0636\n",
      "Epoch 83/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0587 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0564\n",
      "Epoch 84/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0553 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0629\n",
      "Epoch 85/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0535 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0569\n",
      "Epoch 86/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0539 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0552\n",
      "Epoch 87/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0041 - mse: 0.0041 - mae: 0.0506 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0606\n",
      "Epoch 88/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0545 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0624\n",
      "Epoch 89/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0594 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0612\n",
      "Epoch 90/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0597 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0731\n",
      "Epoch 91/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0517 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0580\n",
      "Epoch 92/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0484 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0507\n",
      "Epoch 93/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0482 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0520\n",
      "Epoch 94/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0449 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0530\n",
      "Epoch 95/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0467 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0551\n",
      "Epoch 96/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0537 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0693\n",
      "Epoch 97/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0545 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0652\n",
      "Epoch 98/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0550 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0633\n",
      "Epoch 99/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0555 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0684\n",
      "Epoch 100/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0591 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0515\n",
      "Epoch 101/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0452 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0506\n",
      "Epoch 102/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0488 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0793\n",
      "Epoch 103/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0612 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0604\n",
      "Epoch 104/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0579 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0512\n",
      "Epoch 105/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0490 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0570\n",
      "Epoch 106/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0444 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0541\n",
      "Epoch 107/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0450 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0556\n",
      "Epoch 108/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0549 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0800\n",
      "Epoch 109/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0718 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0597\n",
      "Epoch 110/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0530 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0581\n",
      "Epoch 111/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0553 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0562\n",
      "Epoch 112/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0041 - mse: 0.0041 - mae: 0.0509 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0534\n",
      "Epoch 113/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0484 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0537\n",
      "Epoch 114/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0439 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0507\n",
      "Epoch 115/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0458 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0511\n",
      "Epoch 116/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0034 - mse: 0.0034 - mae: 0.0459 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0575\n",
      "Epoch 117/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0495 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0599\n",
      "Epoch 118/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0465 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0559\n",
      "Epoch 119/250\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0484 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0501\n",
      "Epoch 120/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0034 - mse: 0.0034 - mae: 0.0455 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0558\n",
      "Epoch 121/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0440 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0511\n",
      "Epoch 122/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0443 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0679\n",
      "Epoch 123/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0538 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0617\n",
      "Epoch 124/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0483 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0538\n",
      "Epoch 125/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0473 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0529\n",
      "Epoch 126/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0445 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0675\n",
      "Epoch 127/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0554 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0577\n",
      "Epoch 128/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0562 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0494\n",
      "Epoch 129/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0577 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0611\n",
      "Epoch 130/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0041 - mse: 0.0041 - mae: 0.0506 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0511\n",
      "Epoch 131/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0424 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0504\n",
      "Epoch 132/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0470 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0549\n",
      "Epoch 133/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0034 - mse: 0.0034 - mae: 0.0457 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0591\n",
      "Epoch 134/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0485 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0530\n",
      "Epoch 135/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0445 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0599\n",
      "Epoch 136/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0419 - val_loss: 0.0035 - val_mse: 0.0035 - val_mae: 0.0479\n",
      "Epoch 137/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0413 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0512\n",
      "Epoch 138/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0427 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0534\n",
      "Epoch 139/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0497 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0504\n",
      "Epoch 140/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0449 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0508\n",
      "Epoch 141/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.0026 - mae: 0.0404 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0509\n",
      "Epoch 142/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0401 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0537\n",
      "Epoch 143/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0477 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0671\n",
      "Epoch 144/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0041 - mse: 0.0041 - mae: 0.0514 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0499\n",
      "Epoch 145/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0435 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0580\n",
      "Epoch 146/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0442 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0476\n",
      "Epoch 147/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0434 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0585\n",
      "Epoch 148/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0451 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0590\n",
      "Epoch 149/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0465 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0512\n",
      "Epoch 150/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0434 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0547\n",
      "Epoch 151/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0475 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0495\n",
      "Epoch 152/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0434 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0508\n",
      "Epoch 153/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0479 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0715\n",
      "Epoch 154/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0504 - val_loss: 0.0035 - val_mse: 0.0035 - val_mae: 0.0473\n",
      "Epoch 155/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0422 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0502\n",
      "Epoch 156/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0428 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0584\n",
      "Epoch 157/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0444 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0561\n",
      "Epoch 158/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0458 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0529\n",
      "Epoch 159/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0428 - val_loss: 0.0036 - val_mse: 0.0036 - val_mae: 0.0489\n",
      "Epoch 160/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0413 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0475\n",
      "Epoch 161/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0421 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0546\n",
      "Epoch 162/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0427 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0499\n",
      "Epoch 163/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0412 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0530\n",
      "Epoch 164/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0435 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0619\n",
      "Epoch 165/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0479 - val_loss: 0.0036 - val_mse: 0.0036 - val_mae: 0.0481\n",
      "Epoch 166/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0411 - val_loss: 0.0035 - val_mse: 0.0035 - val_mae: 0.0476\n",
      "Epoch 167/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.0026 - mae: 0.0397 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0538\n",
      "Epoch 168/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0406 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0571\n",
      "Epoch 169/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0498 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0607\n",
      "Epoch 170/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0458 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0502\n",
      "Epoch 171/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0040 - mse: 0.0040 - mae: 0.0506 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0811\n",
      "Epoch 172/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0567 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0620\n",
      "Epoch 173/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0492 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0536\n",
      "Epoch 174/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0421 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0505\n",
      "Epoch 175/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0439 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0529\n",
      "Epoch 176/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0439 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0567\n",
      "Epoch 177/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0485 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0557\n",
      "Epoch 178/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0416 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0529\n",
      "Epoch 179/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0443 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0505\n",
      "Epoch 180/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.0026 - mae: 0.0400 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0541\n",
      "Epoch 181/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0025 - mse: 0.0025 - mae: 0.0392 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0472\n",
      "Epoch 182/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0406 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0496\n",
      "Epoch 183/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0040 - mse: 0.0040 - mae: 0.0499 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0474\n",
      "Epoch 184/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0447 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0509\n",
      "Epoch 185/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0407 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0533\n",
      "Epoch 186/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0459 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0568\n",
      "Epoch 187/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.0026 - mae: 0.0404 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0543\n",
      "Epoch 188/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0477 - val_loss: 0.0085 - val_mse: 0.0085 - val_mae: 0.0772\n",
      "Epoch 189/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0568 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0556\n",
      "Epoch 190/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0510 - val_loss: 0.0036 - val_mse: 0.0036 - val_mae: 0.0483\n",
      "Epoch 191/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0431 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0510\n",
      "Epoch 192/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0442 - val_loss: 0.0036 - val_mse: 0.0036 - val_mae: 0.0497\n",
      "Epoch 193/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0025 - mse: 0.0025 - mae: 0.0398 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0508\n",
      "Epoch 194/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0407 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0565\n",
      "Epoch 195/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0449 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0539\n",
      "Epoch 196/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0396 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0518\n",
      "Epoch 197/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0415 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0507\n",
      "Epoch 198/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0410 - val_loss: 0.0036 - val_mse: 0.0036 - val_mae: 0.0491\n",
      "Epoch 199/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0025 - mse: 0.0025 - mae: 0.0398 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0479\n",
      "Epoch 200/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.0026 - mae: 0.0397 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0543\n",
      "Epoch 201/250\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0416 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0472\n",
      "Epoch 202/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.0024 - mae: 0.0388 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0587\n",
      "Epoch 203/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0447 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0583\n",
      "Epoch 204/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0439 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0525\n",
      "Epoch 205/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0451 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0499\n",
      "Epoch 206/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0418 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0511\n",
      "Epoch 207/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0456 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0555\n",
      "Epoch 208/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0493 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0697\n",
      "Epoch 209/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0506 - val_loss: 0.0035 - val_mse: 0.0035 - val_mae: 0.0482\n",
      "Epoch 210/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0438 - val_loss: 0.0104 - val_mse: 0.0104 - val_mae: 0.0869\n",
      "Epoch 211/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0592 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0592\n",
      "Epoch 212/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0502 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0649\n",
      "Epoch 213/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0459 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0532\n",
      "Epoch 214/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0025 - mse: 0.0025 - mae: 0.0392 - val_loss: 0.0035 - val_mse: 0.0035 - val_mae: 0.0477\n",
      "Epoch 215/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0023 - mse: 0.0023 - mae: 0.0381 - val_loss: 0.0035 - val_mse: 0.0035 - val_mae: 0.0488\n",
      "Epoch 216/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.0024 - mae: 0.0385 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0517\n",
      "Epoch 217/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0406 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0528\n",
      "Epoch 218/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0471 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0628\n",
      "Epoch 219/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0450 - val_loss: 0.0033 - val_mse: 0.0033 - val_mae: 0.0465\n",
      "Epoch 220/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0410 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0559\n",
      "Epoch 221/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0451 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0807\n",
      "Epoch 222/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0034 - mse: 0.0034 - mae: 0.0460 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0523\n",
      "Epoch 223/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0025 - mse: 0.0025 - mae: 0.0397 - val_loss: 0.0032 - val_mse: 0.0032 - val_mae: 0.0471\n",
      "Epoch 224/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0434 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0499\n",
      "Epoch 225/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0412 - val_loss: 0.0035 - val_mse: 0.0035 - val_mae: 0.0480\n",
      "Epoch 226/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.0024 - mae: 0.0389 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0542\n",
      "Epoch 227/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0465 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0514\n",
      "Epoch 228/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0432 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0514\n",
      "Epoch 229/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0429 - val_loss: 0.0035 - val_mse: 0.0035 - val_mae: 0.0477\n",
      "Epoch 230/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0419 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0472\n",
      "Epoch 231/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0448 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0578\n",
      "Epoch 232/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0040 - mse: 0.0040 - mae: 0.0505 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0489\n",
      "Epoch 233/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0405 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0531\n",
      "Epoch 234/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0410 - val_loss: 0.0033 - val_mse: 0.0033 - val_mae: 0.0462\n",
      "Epoch 235/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.0024 - mae: 0.0378 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0561\n",
      "Epoch 236/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0433 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0509\n",
      "Epoch 237/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0399 - val_loss: 0.0035 - val_mse: 0.0035 - val_mae: 0.0486\n",
      "Epoch 238/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0025 - mse: 0.0025 - mae: 0.0394 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0523\n",
      "Epoch 239/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0452 - val_loss: 0.0035 - val_mse: 0.0035 - val_mae: 0.0482\n",
      "Epoch 240/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0404 - val_loss: 0.0033 - val_mse: 0.0033 - val_mae: 0.0470\n",
      "Epoch 241/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0422 - val_loss: 0.0033 - val_mse: 0.0033 - val_mae: 0.0471\n",
      "Epoch 242/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0026 - mse: 0.0026 - mae: 0.0401 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0619\n",
      "Epoch 243/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0423 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0493\n",
      "Epoch 244/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.0024 - mae: 0.0376 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0547\n",
      "Epoch 245/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0025 - mse: 0.0025 - mae: 0.0388 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0510\n",
      "Epoch 246/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0420 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0498\n",
      "Epoch 247/250\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0410 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0685\n",
      "Epoch 248/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0496 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0609\n",
      "Epoch 249/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0036 - mse: 0.0036 - mae: 0.0474 - val_loss: 0.0033 - val_mse: 0.0033 - val_mae: 0.0471\n",
      "Epoch 250/250\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.0026 - mae: 0.0397 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0535\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "klXyWf9iUFXj",
    "outputId": "f40abd50-c66d-4e91-a3a9-60608b595424"
   },
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "k6bvBqGfUFXj",
    "outputId": "da206c73-7740-4910-a4b3-4092ae7710f7"
   },
   "source": [
    "print(history.history.keys())\n",
    "plt.plot(history.history['mse'])\n",
    "plt.plot(history.history['mae'])\n",
    "plt.title('model performance')\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['mse', 'mae'], loc='upper left')\n",
    "plt.show()\n"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IljM30huUFXj",
    "outputId": "63a87972-0832-4539-bd59-765c6a39bc6a"
   },
   "source": [
    "X_new = np.array([[-2.2578408616907195, -0.7619484936348196, -2.5780332793928573,\n",
    "                  -1.9008068681873072, -1.3986096481100645, -1.42322851369181, -3.6418724785872727,\n",
    "                  -3.9060410970576713, -1.7554032607606425, -1.3943150129620527, -1.3894138552044868,\n",
    "                  -1.3483989533215142, -0.8749243064663377, -0.4022884310098427, -2.6667544718571463,\n",
    "                  -1.907681185475232, -1.9270714623297287, -1.8745383118848733,\n",
    "                  -1.8770428706065658, -1.4066279663766306, -1.4222819925666383, -1.9167216073576925,\n",
    "                  -1.8937499044421475, -1.925058900708228, -1.4048608587821392, -1.3864975278692442,\n",
    "                  -1.3857455114766448, -0.925015121858419, -0.4127990174344398, -2.8373837683558767,\n",
    "                  -1.4360815700998038]])\n",
    "X_new = scalar_x.transform(X_new)\n",
    "y_new = model.predict(X_new)\n",
    "#invert normalize\n",
    "y_new = scalar_y.inverse_transform(y_new)\n",
    "X_new = scalar_x.inverse_transform(X_new)\n",
    "print(\"X=%s, Predicted=%s\" % (X_new[0], y_new[0]))\n",
    "nn_predictions = []\n",
    "for value in X:\n",
    "    X_new = np.array([value])\n",
    "    X_new = scalar_x.transform(X_new)\n",
    "    y_new = model.predict(X_new)\n",
    "    #invert normalize\n",
    "    y_new = scalar_y.inverse_transform(y_new)\n",
    "    X_new = scalar_x.inverse_transform(X_new)\n",
    "    nn_predictions.append(y_new[0])"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X=[-2.25784086 -0.76194849 -2.57803328 -1.90080687 -1.39860965 -1.42322851\n",
      " -3.64187248 -3.9060411  -1.75540326 -1.39431501 -1.38941386 -1.34839895\n",
      " -0.87492431 -0.40228843 -2.66675447 -1.90768119 -1.92707146 -1.87453831\n",
      " -1.87704287 -1.40662797 -1.42228199 -1.91672161 -1.8937499  -1.9250589\n",
      " -1.40486086 -1.38649753 -1.38574551 -0.92501512 -0.41279902 -2.83738377\n",
      " -1.43608157], Predicted=[0.6313095]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ioPLT4mWUFXk"
   },
   "source": [
    "team_arr = (team_season.team == \"BOS\").values"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "8VngsmqDUFXk"
   },
   "source": [
    "year_arr = (team_season.year == int(\"1946\")).values"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qg_IiNCRUFXk",
    "outputId": "3d209315-8c73-410a-a1f2-bc7ecec02e84"
   },
   "source": [
    "count = 0\n",
    "index = -1\n",
    "for value in team_arr:\n",
    "    if team_arr[count]:\n",
    "        if year_arr[count]:\n",
    "            index = count\n",
    "    count = count + 1\n",
    "print(index)"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L0ESLvOyf-Le"
   },
   "source": [
    "X = StandardScaler().fit_transform(team_season_dropped)"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Znxuod38UFXl",
    "outputId": "c3f41de0-5e6c-49f5-ada1-7014b4b15a0c"
   },
   "source": [
    "team_season_dropped = X\n",
    "print(X[index].tolist())"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-3.0965134852746914, -1.9924140292745167, -3.003851013800846, -2.345001825632069, -1.399199156944721, -1.4238283992879754, -3.6434075144735982, -3.8129324104116877, -2.8369461426683933, -1.3949027116237493, -1.3899994880461737, -1.3489672985325565, -0.875293083851087, -0.4024579941073652, -3.4914856553243525, -1.9084852660949108, -1.92788371587995, -1.875328422904946, -1.877834037288948, -1.4072208549030147, -1.4228814792076179, -1.9175294984820261, -1.8945481130779862, -1.9258703059711113, -1.4054530024791527, -1.3870819314896607, -1.3863295981249126, -0.9254050123380538, -0.4129730107055733, -3.269244828883868, -1.4366868732120759]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "at8e3rNWUFXl"
   },
   "source": [
    "#save neural network here\n",
    "model.save('NNpredictionModel.h5')"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43EE-_nXUFXl",
    "outputId": "20556caf-9c12-4e96-a4f1-c88c0f938067"
   },
   "source": [
    "X_new = np.array([team_season_dropped[index].tolist()])\n",
    "X_new = scalar_x.transform(X_new)\n",
    "y_new = model.predict(X_new)\n",
    "#invert normalize\n",
    "y_new = scalar_y.inverse_transform(y_new)\n",
    "X_new = scalar_x.inverse_transform(X_new)\n",
    "print(\"X=%s, Predicted=%s\" % (X_new[0], y_new[0]))"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X=[-3.09651349 -1.99241403 -3.00385101 -2.34500183 -1.39919916 -1.4238284\n",
      " -3.64340751 -3.81293241 -2.83694614 -1.39490271 -1.38999949 -1.3489673\n",
      " -0.87529308 -0.40245799 -3.49148566 -1.90848527 -1.92788372 -1.87532842\n",
      " -1.87783404 -1.40722085 -1.42288148 -1.9175295  -1.89454811 -1.92587031\n",
      " -1.405453   -1.38708193 -1.3863296  -0.92540501 -0.41297301 -3.26924483\n",
      " -1.43668687], Predicted=[0.32318163]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "WRuoOnDNUFXl"
   },
   "source": [
    "neural_Network_Results = []\n",
    "for value in team_season_dropped:\n",
    "    X_new = np.array([value.tolist()])\n",
    "    X_new = scalar_x.transform(X_new)\n",
    "    y_new = model.predict(X_new)\n",
    "    #invert normalize\n",
    "    y_new = scalar_y.inverse_transform(y_new)\n",
    "    X_new = scalar_x.inverse_transform(X_new)\n",
    "    neural_Network_Results.append(y_new[0])"
   ],
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUkaSL8NUFXl",
    "outputId": "6520356f-2e33-4285-9b4c-143472e66675"
   },
   "source": [
    "print(len(neural_Network_Results))"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1187\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AClYsko4UFXm",
    "outputId": "5e8959f6-2b0e-42b5-a26c-699b83bb1aa7"
   },
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "X = X\n",
    "y = Y\n",
    "lm = linear_model.LinearRegression()\n",
    "lin_model = lm.fit(X, y)\n",
    "multi_lin_predictions = lin_model.predict(X)\n",
    "print(multi_lin_predictions)\n"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.36881834 0.58872271 0.46709683 ... 0.45662928 0.34614944 0.49810837]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qkw2NKBAVmov",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1abb2e56-9749-4ace-912b-6f104fae42e3"
   },
   "source": [
    "from math import exp\n",
    "\n",
    "print(\"Enter the first team's name: \")\n",
    "team_1_name = input() #input from user\n",
    "print(\"Enter the year for the first team: \")\n",
    "year_1 = input() # input from user\n",
    "\n",
    "print(\"Enter the second team's name: \")\n",
    "team_2_name =  input() #input from user\n",
    "print(\"Enter the year for the second team: \")\n",
    "year_2 = input() # input from user\n",
    "\n"
   ],
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the first team's name: \n",
      "bos\n",
      "Enter the year for the first team: \n",
      "1946\n",
      "Enter the second team's name: \n",
      "ch1\n",
      "Enter the year for the second team: \n",
      "1946\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "alCul-8wWUB8"
   },
   "source": [
    "def getSquareRoot():\n",
    "    team_arr = (team_season.team == str(team_1_name).upper())\n",
    "    year_arr = (team_season.year == int(year_1)).values\n",
    "\n",
    "    count = 0\n",
    "    index = -1\n",
    "    for _ in team_arr:\n",
    "        if team_arr[count]:\n",
    "            if year_arr[count]:\n",
    "                index = count\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    print(index)\n",
    "    print(team_season_dropped[index].tolist())\n",
    "\n",
    "    X_new = np.array([team_season_dropped[index].tolist()])\n",
    "    LM_X = X_new\n",
    "    X_new = scalar_x.transform(X_new)\n",
    "\n",
    "    y_new = model.predict(X_new)\n",
    "    #invert normalize\n",
    "    y_new = scalar_y.inverse_transform(y_new)\n",
    "    X_new = scalar_x.inverse_transform(X_new)\n",
    "    print(\"X=%s, Predicted=%s\" % (X_new[0], y_new[0]))\n",
    "\n",
    "    team2_arr = (team_season.team == str(team_2_name).upper()).values\n",
    "\n",
    "    year2_arr = (team_season.year == int(year_2)).values\n",
    "\n",
    "    count = 0\n",
    "    index2 = -1\n",
    "    for _ in team2_arr:\n",
    "      if team2_arr[count]:\n",
    "        if year2_arr[count]:\n",
    "            index2 = count\n",
    "      count += 1\n",
    "    print(index2)\n",
    "    print(team_season_dropped[index2].tolist())\n",
    "\n",
    "    X_new2 = np.array([team_season_dropped[index2].tolist()])\n",
    "    LMX_2 = X_new2\n",
    "    X_new2 = scalar_x.transform(X_new2)\n",
    "    y_new2 = model.predict(X_new2)\n",
    "    #invert normalize\n",
    "    y_new2 = scalar_y.inverse_transform(y_new2)\n",
    "    X_new2 = scalar_x.inverse_transform(X_new2)\n",
    "    print(\"X=%s, Predicted=%s\" % (X_new2[0], y_new2[0]))\n",
    "\n",
    "    prob1 = exp(float(y_new[0][0])) / (exp(float(y_new[0][0])) + exp(float(y_new2[0][0])))\n",
    "    print(prob1)\n",
    "\n",
    "    prob2 = exp(float(y_new2[0][0])) / (exp(float(y_new[0][0])) + exp(float(y_new2[0][0])))\n",
    "    print(prob2)\n",
    "\n",
    "    print('For the Neural Network...')\n",
    "    print('The normalized probability for team one winning is: ' + str(prob1))\n",
    "    print('The normalized probability for team two winning is: ' + str(prob2))\n",
    "\n",
    "    print(\"LM prediction is\" + str(multi_lin_predictions[index]))\n",
    "\n",
    "    LM_Pred_1 = float(multi_lin_predictions[index])\n",
    "    LM_Pred_2 = float(multi_lin_predictions[index2])\n",
    "\n",
    "    normalized_LM_1 = exp(LM_Pred_1) / (exp(LM_Pred_1) + exp(LM_Pred_2))\n",
    "    print(normalized_LM_1 )\n",
    "\n",
    "    normalized_LM_2 = exp(LM_Pred_2) / (exp(LM_Pred_1) + exp(LM_Pred_2))\n",
    "    print(normalized_LM_2 )\n",
    "\n",
    "    print('For the Multiple Linear Regression Model... ')\n",
    "\n",
    "    print('The normalized probability for team one winning is: '+str(normalized_LM_1))\n",
    "    print('The normalized probability for team two winning is: '+str(normalized_LM_2))\n",
    "\n",
    "\n",
    "    final_team1 = prob1 + normalized_LM_1\n",
    "    final_team2 = prob2 + normalized_LM_2\n",
    "\n",
    "    Final_normalized_LM_1 = exp(final_team1) / (exp(final_team1) + exp(final_team2))\n",
    "    print(normalized_LM_1 )\n",
    "\n",
    "    Final_normalized_LM_2 = exp(final_team2) / (exp(final_team1) + exp(final_team2))\n",
    "    print(normalized_LM_2 )\n",
    "\n",
    "\n",
    "    print('For the Ensemble of Both Models...')\n",
    "\n",
    "    print('The normalized probability for team one winning is: '+str(Final_normalized_LM_1))\n",
    "\n",
    "    print('The normalized probability for team two winning is: ' + str(Final_normalized_LM_2))\n",
    "\n",
    "\n",
    "    winner = \"\"\n",
    "    if float(Final_normalized_LM_1)>float(Final_normalized_LM_2):\n",
    "        winner = \"The Ensemble result shows that team 1 wins!\"\n",
    "    elif float(Final_normalized_LM_2) > float(Final_normalized_LM_1):\n",
    "        winner = \"The Ensemble result shows that team 2 wins!\"\n",
    "\n",
    "    print(winner)\n"
   ],
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sm7g6py_V99t",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d8641bad-156b-434a-a2d3-59334ffc967c"
   },
   "source": [
    "getSquareRoot()"
   ],
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "[-3.0965134852746914, -1.9924140292745167, -3.003851013800846, -2.345001825632069, -1.399199156944721, -1.4238283992879754, -3.6434075144735982, -3.8129324104116877, -2.8369461426683933, -1.3949027116237493, -1.3899994880461737, -1.3489672985325565, -0.875293083851087, -0.4024579941073652, -3.4914856553243525, -1.9084852660949108, -1.92788371587995, -1.875328422904946, -1.877834037288948, -1.4072208549030147, -1.4228814792076179, -1.9175294984820261, -1.8945481130779862, -1.9258703059711113, -1.4054530024791527, -1.3870819314896607, -1.3863295981249126, -0.9254050123380538, -0.4129730107055733, -3.269244828883868, -1.4366868732120759]\n",
      "X=[-3.09651349 -1.99241403 -3.00385101 -2.34500183 -1.39919916 -1.4238284\n",
      " -3.64340751 -3.81293241 -2.83694614 -1.39490271 -1.38999949 -1.3489673\n",
      " -0.87529308 -0.40245799 -3.49148566 -1.90848527 -1.92788372 -1.87532842\n",
      " -1.87783404 -1.40722085 -1.42288148 -1.9175295  -1.89454811 -1.92587031\n",
      " -1.405453   -1.38708193 -1.3863296  -0.92540501 -0.41297301 -3.26924483\n",
      " -1.43668687], Predicted=[0.32318163]\n",
      "1\n",
      "[-2.2587925333290553, -0.7622696521290119, -2.5791199109603484, -1.9016080513074731, -1.399199156944721, -1.4238283992879754, -3.6434075144735982, -3.9076874790473566, -1.7561431567938186, -1.3949027116237493, -1.3899994880461737, -1.3489672985325565, -0.875293083851087, -0.4024579941073652, -2.667878499081709, -1.9084852660949108, -1.92788371587995, -1.875328422904946, -1.877834037288948, -1.4072208549030147, -1.4228814792076179, -1.9175294984820261, -1.8945481130779862, -1.9258703059711113, -1.4054530024791527, -1.3870819314896607, -1.3863295981249126, -0.9254050123380538, -0.4129730107055733, -2.8385797152028123, -1.4366868732120759]\n",
      "X=[-2.25879253 -0.76226965 -2.57911991 -1.90160805 -1.39919916 -1.4238284\n",
      " -3.64340751 -3.90768748 -1.75614316 -1.39490271 -1.38999949 -1.3489673\n",
      " -0.87529308 -0.40245799 -2.6678785  -1.90848527 -1.92788372 -1.87532842\n",
      " -1.87783404 -1.40722085 -1.42288148 -1.9175295  -1.89454811 -1.92587031\n",
      " -1.405453   -1.38708193 -1.3863296  -0.92540501 -0.41297301 -2.83857972\n",
      " -1.43668687], Predicted=[0.6317259]\n",
      "0.4234701037709419\n",
      "0.5765298962290581\n",
      "For the Neural Network...\n",
      "The normalized probability for team one winning is: 0.4234701037709419\n",
      "The normalized probability for team two winning is: 0.5765298962290581\n",
      "LM prediction is0.36881833706069167\n",
      "0.445244385325036\n",
      "0.5547556146749639\n",
      "For the Multiple Linear Regression Model... \n",
      "The normalized probability for team one winning is: 0.445244385325036\n",
      "The normalized probability for team two winning is: 0.5547556146749639\n",
      "0.445244385325036\n",
      "0.5547556146749639\n",
      "For the Ensemble of Both Models...\n",
      "The normalized probability for team one winning is: 0.4347317994484929\n",
      "The normalized probability for team two winning is: 0.5652682005515072\n",
      "The Ensemble result shows that team 2 wins!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UcRLFfUnBPkG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9c337c4c-329b-42c4-f147-fc2656082957"
   },
   "source": [
    "from sklearn import linear_model\n",
    "y = Y\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X,y)\n",
    "multi_lin_predictions = lm.predict(X)\n",
    "print(multi_lin_predictions[1])"
   ],
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5887227079851802\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H-KqzkVoBRVO"
   },
   "source": [
    "lin_regr = linear_model.LinearRegression()\n",
    "lin_regr.fit(X, y)\n",
    "predicted_values = []\n",
    "count = 0\n",
    "for i in X:\n",
    " lin_predicted_prob = lin_regr.predict([i])\n",
    " predicted_values.append(lin_predicted_prob)"
   ],
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yvDXemTkBUJS"
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(X)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
    "from sklearn import linear_model\n",
    "x = principalDf\n",
    "y = Y\n",
    "lin_regr = linear_model.LinearRegression()\n",
    "lin_regr.fit(x.values, y)\n",
    "simple_LM = []\n",
    "count = 0\n",
    "for i in x.values:\n",
    " lin_predicted_prob = lin_regr.predict([i])\n",
    " simple_LM.append(lin_predicted_prob)"
   ],
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Nns2m2XHBbpE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b7750402-d2ad-4a8f-ea36-d550f1bcaa34"
   },
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mseSLR = mean_squared_error(np.array(simple_LM), y)\n",
    "mseMLR = mean_squared_error(np.array(multi_lin_predictions), y)\n",
    "mseNN = mean_squared_error(np.array(neural_Network_Results), y)\n",
    "maeSLR = mean_absolute_error(np.array(simple_LM), y)\n",
    "maeMLR = mean_absolute_error(np.array(multi_lin_predictions), y)\n",
    "maeNN = mean_absolute_error(np.array(neural_Network_Results), y)\n",
    "print(\"The Mean Squared Error for the Simple Linear Regression model is \",mseSLR)\n",
    "print(\"The Mean Absolute Error for the Simple Linear Regression model is \",maeSLR)\n",
    "print(\"The variance for the Simple Linear Prediction Model is \",np.var(simple_LM))\n",
    "print(\"\\nThe Mean Squared Error for the Multiple Linear Regression model is \",mseMLR)\n",
    "print(\"The Mean Absolute Error for the Multiple Linear Regression model is \",maeMLR)\n",
    "print(\"The variance for the Multiple Linear Prediction Model is \",np.var(multi_lin_predictions))\n",
    "print(\"\\nThe Mean Squared Error for the Probabilistic Regression Neural Network is \",mseNN)\n",
    "print(\"The Mean Absolute Error for the Probabilistic Regression Neural Network is \",maeNN)\n",
    "print(\"The variance for the Probabilistic Regression Neural Network is \",np.var(neural_Network_Results))\n"
   ],
   "execution_count": 45,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Mean Squared Error for the Simple Linear Regression model is  0.022158138051408295\n",
      "The Mean Absolute Error for the Simple Linear Regression model is  0.12193389814976235\n",
      "The variance for the Simple Linear Prediction Model is  6.0021970785201535e-05\n",
      "\n",
      "The Mean Squared Error for the Multiple Linear Regression model is  0.0017305552240043215\n",
      "The Mean Absolute Error for the Multiple Linear Regression model is  0.03243356680911767\n",
      "The variance for the Multiple Linear Prediction Model is  0.020487604798189347\n",
      "\n",
      "The Mean Squared Error for the Probabilistic Regression Neural Network is  0.002009837569197645\n",
      "The Mean Absolute Error for the Probabilistic Regression Neural Network is  0.036156740150859436\n",
      "The variance for the Probabilistic Regression Neural Network is  0.020392904\n"
     ]
    }
   ]
  }
 ]
}